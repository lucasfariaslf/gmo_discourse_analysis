{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMO Discourse Analysis \n",
    "\n",
    "This notebooks presents an exercise for the validation of an automated methodology aimed at classifying textual data into different types of discourses in the context of GMO's, based on classical text mining techniques such as _bag of words_ standard _word vectorization_ and _tf-idf_. The following Python libraries will be used:\n",
    "\n",
    " - [**scikit-learn**] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Vanderplas, J. (2011). Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), 2825-2830.\n",
    " \n",
    " - [**nltk**] Loper, E., & Bird, S. (2002, July). NLTK: The natural language toolkit. In Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics-Volume 1 (pp. 63-70). Association for Computational Linguistics.\n",
    "\n",
    "The theoretical background is mainly based on the following texts:\n",
    "\n",
    " - Fontoura, Y. S. D. R. D. (2015). International civil society actors in Genetically Modificied Organisms as a field of struggle: a neo-gramscian study in Brazil and the United Kingdom (Doctoral dissertation).\n",
    " \n",
    " - Levy, David & Reinecke, Juliane & Manning, Stephan. (2016). The Political Dynamics of Sustainable Coffee: Contested Value Regimes and the Transformation of Sustainability. Journal of Management Studies. 53. 364-401. 10.1111/joms.12144. \n",
    "\n",
    "Author: Lucas Farias\n",
    "\n",
    "Supervision: Yuna Fontoura and Jefferson Santos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import collections, re\n",
    "import nltk\n",
    "import glob\n",
    "from tqdm import tqdm_notebook\n",
    "import regex\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions to clean text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_unicode(data):\n",
    "    \n",
    "    '''\n",
    "    transforms text data to unicode\n",
    "    ''' \n",
    "    \n",
    "    if type(data)==str:\n",
    "        data = data\n",
    "        print(type(data))\n",
    "    else:    \n",
    "        try:\n",
    "            data = data.decode('utf-8')\n",
    "        except (UnicodeDecodeError, UnicodeEncodeError):\n",
    "            try:\n",
    "                data = data.decode('iso-8859-1')\n",
    "            except (UnicodeDecodeError, UnicodeEncodeError):\n",
    "                try:\n",
    "                    data = data.decode('latin-1')\n",
    "                except (UnicodeDecodeError, UnicodeEncodeError):\n",
    "                    data = data\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonlatin(string): \n",
    "    \n",
    "    '''\n",
    "    removes non-latin characters and newlines\n",
    "    '''\n",
    "    \n",
    "    new_chars = []\n",
    "    for char in string:\n",
    "        if char == '\\n':\n",
    "            new_chars.append(' ')\n",
    "            continue\n",
    "        try:\n",
    "            if unicodedata.name(unicode(char)).startswith(('LATIN', 'SPACE')):\n",
    "                new_chars.append(char)\n",
    "        except:\n",
    "            try:\n",
    "                if unicodedata.name(char).startswith(('LATIN', 'SPACE')):\n",
    "                    new_chars.append(char)\n",
    "            except:\n",
    "                continue\n",
    "    return ''.join(new_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of <span style=\"color:red\"> PRO </span> discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current dir\n",
    "articles_path = os.getcwd()+'\\\\articles\\\\repsoy\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(articles_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ddab5cda0048858146abe29abaf4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=367), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "read_files = glob.glob(articles_path + \"\\\\*.txt\")\n",
    "all_articles_path = articles_path + \"all_articles.txt\"\n",
    "\n",
    "with open(all_articles_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for f in tqdm_notebook(read_files):\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as infile:\n",
    "#             print('\\n\\n\\n'+ 10*'=' + '\\n\\n\\n', infile.read())\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(all_articles_path, 'r', encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = remove_nonlatin(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
